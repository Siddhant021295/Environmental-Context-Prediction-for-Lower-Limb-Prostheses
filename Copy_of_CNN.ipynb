{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VUjfmO_K5Vsn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import stats\n",
    "from torchvision.io import read_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def funition_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "funition_seed(42)\n",
    "\n",
    "#Reading Data upscaled the data removed the 1st entry from the X values. \n",
    "dtype = {'subject_name'  : np.str_\n",
    ",'subject_instance' : np.str_\n",
    ",'timestamp'        : np.float64\n",
    ",'x_accelerometer'   : np.float64\n",
    ",'y_accelerometer'  : np.float64\n",
    ",'z_accelerometer'  : np.float64\n",
    ",'x_gyroscope'      : np.float64\n",
    ",'y_gyroscope'      : np.float64\n",
    ",'z_gyroscope'      : np.float64\n",
    ",'labels'           : np.int64}\n",
    "data_all=pd.read_csv('./data_all.csv',dtype=dtype)\n",
    "\n",
    "data_all.head()\n",
    "\n",
    "# Creating a New Column for identifing subject, name ,instance in the same column \n",
    "data_all['subject_name_instance'] = data_all['subject_name']+'_'+data_all['subject_instance']\n",
    "\n",
    "# Finding the name of all the instances to interate over.\n",
    "subject_name_instances=data_all['subject_name_instance'].unique()\n",
    "\n",
    "#printing the first 5 columns\n",
    "data_all.head()\n",
    "\n",
    "# Data Preparation function\n",
    "def data_prep(window_size,step_size,data_all,subject_name_instances):\n",
    "  x_list_acc = []\n",
    "  y_list_acc = []\n",
    "  z_list_acc = []\n",
    "  x_list_gyr = []\n",
    "  y_list_gyr = []\n",
    "  z_list_gyr = []\n",
    "  train_labels = []\n",
    "  data=[]\n",
    "  for subject_name_instance in subject_name_instances:\n",
    "    df_train = data_all[data_all['subject_name_instance']==subject_name_instance]\n",
    "    for i in range(0, df_train.shape[0] - window_size, step_size):\n",
    "      xs_acc = df_train['x_accelerometer'].values[i: i + window_size].reshape(1,window_size)\n",
    "      ys_acc = df_train['y_accelerometer'].values[i: i + window_size].reshape(1,window_size)\n",
    "      zs_acc = df_train['z_accelerometer'].values[i: i + window_size].reshape(1,window_size)\n",
    "      xs_gyr = df_train['x_accelerometer'].values[i: i + window_size].reshape(1,window_size)\n",
    "      ys_gyr = df_train['y_accelerometer'].values[i: i + window_size].reshape(1,window_size)\n",
    "      zs_gyr = df_train['z_accelerometer'].values[i: i + window_size].reshape(1,window_size)\n",
    "      label = stats.mode(df_train['labels'][i: i + window_size])[0][0]\n",
    "      data_point=np.vstack((xs_acc,ys_acc,zs_acc,xs_gyr,ys_gyr,zs_gyr))\n",
    "      train_labels.append(label)\n",
    "      data.append(data_point.reshape(1,6,window_size))\n",
    "  return data,train_labels\n",
    "#Deciding the window and training the data\n",
    "window_size,step_size=60,4\n",
    "data,labels = data_prep(window_size,step_size,data_all,subject_name_instances)\n",
    "data_code = str(window_size)+'_'+str(step_size)\n",
    "\n",
    "# Changing the data to np.array \n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "df=pd.DataFrame(zip(data,labels),columns=['data_point','labels'])\n",
    "df['labels'].value_counts()\n",
    "\n",
    "#train\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(df['data_point'],df['labels'], test_size=0.1, random_state=42, shuffle=True, stratify=df['labels'].values)\n",
    "def stratified_with_same_number_of_samples(X_train,Y_train):\n",
    "  min_count= [100000,100000,100000,100000]\n",
    "  data_train=pd.concat([X_train,Y_train],axis = 1)\n",
    "\n",
    "\n",
    "  df_majority_0 = data_train[data_train.iloc[:,-1]==0]\n",
    "  df_minority_3 = data_train[data_train.iloc[:,-1]==3]\n",
    "  df_minority_2 = data_train[data_train.iloc[:,-1]==2]\n",
    "  df_minority_1 = data_train[data_train.iloc[:,-1]==1]\n",
    "  \n",
    "\n",
    "  df_majority_0_sample =   df_majority_0.sample(min_count[0])   \n",
    "  df_majority_3_sample =   df_minority_3.sample(min_count[3],replace= True)   \n",
    "  df_majority_2_sample =   df_minority_2.sample(min_count[2],replace= True)   \n",
    "  df_majority_1_sample =   df_minority_1.sample(min_count[1],replace= True)\n",
    "\n",
    "\n",
    "  data_all_sample = pd.concat([df_majority_0_sample, df_majority_1_sample,df_majority_2_sample,df_majority_3_sample], axis=0)\n",
    "  X_train, Y_train=data_all_sample['data_point'].values, data_all_sample['labels'].values\n",
    "\n",
    "  return X_train,Y_train\n",
    "\n",
    "X_train, Y_train = stratified_with_same_number_of_samples(X_train,Y_train)\n",
    "\n",
    "X_test,Y_test= X_test.values,Y_test.values\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "X_test = np.vstack(X_test)\n",
    "\n",
    "len_train = X_train.shape[0]\n",
    "len_test = X_test.shape[0]\n",
    "print(len_train,len_test)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # Converting RGB [0,255] to Tensor [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes using specified mean and std per channel\n",
    "    transforms.Normalize(( 0.5), (0.5)) \n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data,label,window_size):\n",
    "        self.y_data_point = label\n",
    "        self.x_data_point = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_data_point)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.x_data_point[idx]\n",
    "        label = self.y_data_point[idx]\n",
    "        data = data.reshape(6,window_size)\n",
    "        return data, label\n",
    "\n",
    "    \n",
    "# Size of the batch\n",
    "batch_size = 64\n",
    "train_data = CustomImageDataset(X_train,Y_train,window_size)\n",
    "test_data = CustomImageDataset(X_test,Y_test,window_size)\n",
    "\n",
    "# Selecting the training and test datasets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_features,train_labels  = next(iter(train_loader))\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['Standing/Walking on Solid Ground','Up The Stairs','Down The Stairs','Walking on grass']\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "# Defining the CNN layers architecture\n",
    "class CNN_Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNN_Net, self).__init__()\n",
    "    self.conv1 = nn.Conv1d(6, 12, 5,stride = 1)\n",
    "    self.conv2 = nn.Conv1d(12, 24, 5,stride = 1)\n",
    "    self.conv3 = nn.Conv1d(24, 48, 5,stride = 1)\n",
    "    self.fc1 = nn.Linear(48*48, 256)\n",
    "    self.fc2 = nn.Linear(256, 64)\n",
    "    self.fc3 = nn.Linear(64, 4)\n",
    "  def forward(self, x):\n",
    "    x = x.float() \n",
    "    x = self.conv1(x)                                     \n",
    "    x = F.relu(x)\n",
    "    x = self.conv2(x)                                      \n",
    "    x = F.relu(x)\n",
    "    x = self.conv3(x)                                     \n",
    "    x = F.relu(x)\n",
    "    x = x.view(-1, 48*48)                                 \n",
    "    x = self.fc1(x)                                       \n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)                                       \n",
    "    x = F.relu(x)\n",
    "    x = self.fc3(x)                                       \n",
    "      \n",
    "    return x\n",
    "\n",
    "# Defining the CNN with Batch Normalization layers architecture\n",
    "class Net_CNN_Norm(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net_CNN_Norm, self).__init__()\n",
    "    self.conv1 = nn.Conv1d(6, 12, 5,stride = 1)\n",
    "    self.norm1 = nn.BatchNorm1d(12)\n",
    "    \n",
    "    self.conv2 = nn.Conv1d(12, 24, 5,stride = 1)\n",
    "    self.norm2 = nn.BatchNorm1d(24)\n",
    "    \n",
    "    self.conv3 = nn.Conv1d(24, 48, 5,stride = 1)\n",
    "    self.norm3 = nn.BatchNorm1d(48)\n",
    "    \n",
    "    self.fc1 = nn.Linear(48*48, 256)\n",
    "    self.norm4 = nn.BatchNorm1d(256)\n",
    "    \n",
    "    self.fc2 = nn.Linear(256, 64)\n",
    "    self.norm5 = nn.BatchNorm1d(64)\n",
    "    \n",
    "    self.fc3 = nn.Linear(64, 4)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x.float() \n",
    "    x = self.conv1(x)                                     \n",
    "    x = self.norm1(x)\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.conv2(x)                                      \n",
    "    x = self.norm2(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    x = self.conv3(x)                                     \n",
    "    x = self.norm3(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    x = x.view(-1, 48*48)                                 \n",
    "      \n",
    "    x = self.fc1(x)                                       \n",
    "    x = self.norm4(x)  \n",
    "    x = F.relu(x)\n",
    "      \n",
    "    x = self.fc2(x)                                       \n",
    "    x = self.norm5(x)\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.fc3(x)                                       \n",
    "      \n",
    "    return x\n",
    "\n",
    "# Defining the CNN with Batch Normalization and Dropout layer after every layer architecture\n",
    "class Net_CNN_Norm_Dropout_All(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net_CNN_Norm_Dropout_All, self).__init__()\n",
    "    self.conv1 = nn.Conv1d(6, 12, 5,stride = 1)\n",
    "    self.norm1 = nn.BatchNorm1d(12)\n",
    "    self.conv2 = nn.Conv1d(12, 24, 5,stride = 1)\n",
    "    self.norm2 = nn.BatchNorm1d(24)\n",
    "    self.conv3 = nn.Conv1d(24, 48, 5,stride = 1)\n",
    "    self.norm3 = nn.BatchNorm1d(48)\n",
    "    self.fc1 = nn.Linear(48*48, 256)\n",
    "    self.norm4 = nn.BatchNorm1d(256)\n",
    "    self.fc2 = nn.Linear(256, 64)\n",
    "    self.norm5 = nn.BatchNorm1d(64)\n",
    "    self.drop = nn.Dropout(p =0.1)\n",
    "    self.fc3 = nn.Linear(64, 4)\n",
    "  def forward(self, x):\n",
    "    x = x.float() \n",
    "    x = self.conv1(x)                                     \n",
    "    x = self.norm1(x) \n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.conv2(x)                                      \n",
    "    x = self.norm2(x) \n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x)\n",
    "\n",
    "    x = self.conv3(x)                                     \n",
    "    x = self.norm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x)\n",
    "\n",
    "    x = x.view(-1, 48*48)                                 \n",
    "      \n",
    "    x = self.fc1(x)                                       \n",
    "    x = self.norm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x) \n",
    "      \n",
    "    x = self.fc2(x)                                       \n",
    "    x = self.norm5(x)   \n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.drop(x)\n",
    "    x = self.fc3(x)                                       \n",
    "      \n",
    "    return x\n",
    "\n",
    "\n",
    "# Defining the CNN with Batch Normalization(after every layer) and Dropout (only after fully connected layer) layers architecture\n",
    "class Net_CNN_Norm_Dropout(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net_CNN_Norm_Dropout, self).__init__()\n",
    "    self.conv1 = nn.Conv1d(6, 12, 5,stride = 1)\n",
    "    self.norm1 = nn.BatchNorm1d(12)\n",
    "    self.conv2 = nn.Conv1d(12, 24, 5,stride = 1)\n",
    "    self.norm2 = nn.BatchNorm1d(24)\n",
    "    self.conv3 = nn.Conv1d(24, 48, 5,stride = 1)\n",
    "    self.norm3 = nn.BatchNorm1d(48)\n",
    "    self.fc1 = nn.Linear(48*48, 256)\n",
    "    self.norm4 = nn.BatchNorm1d(256)\n",
    "    self.fc2 = nn.Linear(256, 64)\n",
    "    self.norm5 = nn.BatchNorm1d(64)\n",
    "    self.drop = nn.Dropout(p =0.1)\n",
    "    self.fc3 = nn.Linear(64, 4)\n",
    "  def forward(self, x):\n",
    "    x = x.float() \n",
    "    x = self.conv1(x)                                     \n",
    "    x = self.norm1(x)\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.conv2(x)                                      \n",
    "    x = self.norm2(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    x = self.conv3(x)                                     \n",
    "    x = self.norm3(x)\n",
    "    x = F.relu(x)\n",
    "   \n",
    "    x = x.view(-1, 48*48)                                 \n",
    "\n",
    "    x = self.fc1(x)                                       \n",
    "    x = self.norm4(x)  \n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x)\n",
    "\n",
    "      \n",
    "    x = self.fc2(x)                                       \n",
    "    x = self.norm5(x)   \n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x)\n",
    "    \n",
    "    x = self.fc3(x)                                       \n",
    "      \n",
    "    return x\n",
    "\n",
    "# Defining the CNN with Batch Normalization and Dropout layer after every layer architecture\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "class Net_CNN_LSTM_Norm_Dropout_All(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net_CNN_LSTM_Norm_Dropout_All, self).__init__()\n",
    "    self.conv1 = nn.Conv1d(6, 12, 5,stride = 1,padding = 2)\n",
    "    self.norm1 = nn.BatchNorm1d(12)\n",
    "    self.conv2 = nn.Conv1d(12, 24, 5,stride = 1,padding = 2)\n",
    "    self.norm2 = nn.BatchNorm1d(24)\n",
    "    self.conv3 = nn.Conv1d(24, 48, 5,stride = 1,padding = 2)\n",
    "    self.norm3 = nn.BatchNorm1d(48)\n",
    "    self.lstm = nn.LSTM(48,128,2, batch_first=True)\n",
    "    self.fc1 = nn.Linear(60*128, 256)\n",
    "    self.norm4 = nn.BatchNorm1d(256)\n",
    "    self.fc2 = nn.Linear(256, 64)\n",
    "    self.norm5 = nn.BatchNorm1d(64)\n",
    "    self.drop = nn.Dropout(p =0.1)\n",
    "    self.fc3 = nn.Linear(64, 4)\n",
    "  def forward(self, x):\n",
    "    if flag_cuda:\n",
    "        h0 = torch.zeros(2, x.size(0), 128).cuda()\n",
    "        c0 = torch.zeros(2, x.size(0), 128).cuda()\n",
    "    else:\n",
    "        h0 = torch.zeros(2, x.size(0), 128)\n",
    "        c0 = torch.zeros(2, x.size(0), 128)\n",
    "    x = x.float() \n",
    "    x = self.conv1(x)                                     \n",
    "    x = self.norm1(x) \n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.conv2(x)                                      \n",
    "    x = self.norm2(x) \n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x)\n",
    "\n",
    "    x = self.conv3(x)                                     \n",
    "    x = self.norm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x)\n",
    "    x = x.reshape(-1,60,48)\n",
    "\n",
    "    x, _ = self.lstm(x, (h0,c0))\n",
    "    x = x.reshape(x.shape[0],-1)                              \n",
    "    x = self.fc1(x) \n",
    "\n",
    "    x = self.norm4(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.drop(x) \n",
    "      \n",
    "    x = self.fc2(x)                                       \n",
    "    x = self.norm5(x)   \n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = self.drop(x)\n",
    "    x = self.fc3(x)                                       \n",
    "      \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def validation(valid_losslist,model,test_loader):\n",
    "  valid_loss = 0.0\n",
    "  for data, target in test_loader:\n",
    "      # Moving tensors to GPU if CUDA is available\n",
    "      if flag_cuda:\n",
    "          data, target = data.cuda(), target.cuda()\n",
    "      output = model(data)\n",
    "      loss = criterion(output, target)\n",
    "      valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "  # Calculating average validation losses\n",
    "  valid_loss = valid_loss/len_test\n",
    "  valid_losslist.append(valid_loss)\n",
    "\n",
    "  return valid_losslist,valid_loss,model\n",
    "\n",
    "def training(train_losslist,model,train_loader):\n",
    "  train_loss = 0.0\n",
    "  for data, target in train_loader:\n",
    "      # Moving tensors to GPU if CUDA is available\n",
    "      if flag_cuda:\n",
    "          data, target = data.cuda(), target.cuda()\n",
    "      # Clearing the gradients of all optimized variables\n",
    "      optimizer.zero_grad()\n",
    "  \n",
    "      output = model(data)\n",
    "      # Calculating the batch loss\n",
    "      loss = criterion(output, target)\n",
    "      # Backward pass: compute gradient of loss with respect to parameters\n",
    "      loss.backward()\n",
    "      # Perform a single optimization step (parameter update)\n",
    "      optimizer.step()\n",
    "      # Update training loss\n",
    "      train_loss += loss.item()*data.size(0)\n",
    "  \n",
    "  # Calculating average training losses\n",
    "  train_loss = train_loss/len_train\n",
    "  train_losslist.append(train_loss)\n",
    "  \n",
    "  return train_losslist,train_loss,model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specifying the number of epochs\n",
    "n_epochs = 20\n",
    "\n",
    "def trainNet(model,criterion,n_epochs,flag_cuda,save_model_name,optimizer,train_loader,test_loader):\n",
    "  \n",
    "  # Unpacking the number of epochs to train the model\n",
    "  epochs_list = [*range(1,n_epochs+1)]\n",
    "\n",
    "  # List to store loss to visualize\n",
    "  train_losslist = []\n",
    "  valid_losslist = []\n",
    "  valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "  for epoch in epochs_list:\n",
    "      # Change the mode of the model to training\n",
    "      model.train()\n",
    "      \n",
    "      # Training\n",
    "      train_losslist,train_loss,model = training(train_losslist,model,train_loader)\n",
    "      \n",
    "      # Change the mode of the model to evaluation\n",
    "      model.eval()\n",
    "      \n",
    "      #Evaluation\n",
    "      valid_losslist,valid_loss,model = validation(valid_losslist,model,test_loader)\n",
    "\n",
    "      # Printing training/validation statistics \n",
    "      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "      \n",
    "      # Saving model if validation loss has decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "          torch.save(model.state_dict(),'./Models_weight/'+save_model_name)\n",
    "          valid_loss_min = valid_loss\n",
    "        \n",
    "  return epochs_list, train_losslist, valid_losslist, model\n",
    "\n",
    "def assessNet(model,criterion,loader):\n",
    "  # Tracking test loss and accuracy\n",
    "  test_loss = 0.0\n",
    "  class_correct = list(0. for i in range(len(classes)))\n",
    "  class_total = list(0. for i in range(len(classes)))\n",
    "\n",
    "  # Setting model to evaluate\n",
    "  model.eval()\n",
    "\n",
    "  # Iterating over batches of test data\n",
    "  for data, target in loader:\n",
    "      # Obtaining predictions and loss\n",
    "      if flag_cuda:\n",
    "          data, target = data.cuda(), target.cuda()\n",
    "      output = model(data)\n",
    "      loss = criterion(output, target)\n",
    "      test_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # Converting output probabilities to predicted class\n",
    "      _, pred = torch.max(output, 1)    \n",
    "      # Comparing predictions to true label\n",
    "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "      correct = np.squeeze(correct_tensor.numpy()) if not flag_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "      # Calculating test accuracy for each object class\n",
    "      for i in range(len(correct)):\n",
    "          label = target.data[i]\n",
    "          class_correct[label] += correct[i].item()\n",
    "          class_total[label] += 1\n",
    "\n",
    "  # Computing the average test loss\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  print('Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "  # Computing the class accuracies\n",
    "  for i in range(4):\n",
    "      if class_total[i] > 0:\n",
    "          print('Accuracy of %10s: %2d%% (%2d/%2d)' % (\n",
    "              classes[i], 100 * class_correct[i] / class_total[i],\n",
    "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "      else:\n",
    "          print('Accuracy of %10s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "  # Computing the overall accuracy\n",
    "  print('\\nAccuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "      100. * np.sum(class_correct) / np.sum(class_total),\n",
    "      np.sum(class_correct), np.sum(class_total)))\n",
    "  \n",
    "# Keep track of correct guesses in a confusion matrix\n",
    "label_count=df['labels']\n",
    "n_categories=len(df['labels'].unique())\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "\n",
    "def evaluate_confusion_matrix(model,test_loader):\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # Moving tensors to GPU if CUDA is available\n",
    "        if flag_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        for t, p in zip(target.view(-1), preds.view(-1)):\n",
    "          confusion[t.long(), p.long()] += 1\n",
    "        #print(confusion)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for i in range(n_categories):\n",
    "        confusion[i] = confusion[i] / confusion[i].sum()\n",
    "        accuracy += confusion[i][i]\n",
    "    accuracy /= n_categories\n",
    "\n",
    "    # Displaying the average accuracy\n",
    "    print('Average Macro Accuracy = {:.2f}\\n'.format(accuracy))\n",
    "    return confusion\n",
    "\n",
    "def test_plot(confusion, all_categories):\n",
    "    # Set up plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.numpy())\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "    ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def models():\n",
    "  model_CNN_Net = CNN_Net().float()\n",
    "  model_Net_CNN_Norm = Net_CNN_Norm().float()\n",
    "  model_Net_CNN_Norm_Dropout = Net_CNN_Norm_Dropout().float()\n",
    "  model_Net_CNN_Norm_Dropout_All = Net_CNN_Norm_Dropout_All().float()\n",
    "  model_Net_CNN_LSTM_Norm_Dropout_All = Net_CNN_LSTM_Norm_Dropout_All().float()\n",
    "  return {'model_Net_CNN_LSTM_Norm_Dropout_All':model_Net_CNN_LSTM_Norm_Dropout_All,\n",
    "          'model_CNN_Net':model_CNN_Net,\n",
    "          'model_Net_CNN_Norm' : model_Net_CNN_Norm,\n",
    "          'model_Net_CNN_Norm_Dropout' : model_Net_CNN_Norm_Dropout,\n",
    "          'model_Net_CNN_Norm_Dropout_All': model_Net_CNN_Norm_Dropout_All,}\n",
    "\n",
    "# Create a complete CNN\n",
    "model_data={}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "flag_cuda = torch.cuda.is_available()\n",
    "\n",
    "if not flag_cuda:\n",
    "    print('Using CPU')\n",
    "else:\n",
    "    print('Using GPU')\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "  \n",
    "  # Specifying the loss function\n",
    "  optimizer =optim.SGD(model.parameters(), lr=.001)\n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_SGD_'+model_name+'.pt'\n",
    "  print('################################')\n",
    "  print('Training ',save_model_name,'...')\n",
    "  print('################################')\n",
    "\n",
    "  epochs_list, train_losslist, valid_losslist, model = trainNet(model,criterion,n_epochs,flag_cuda,save_model_name,optimizer,train_loader,test_loader)\n",
    "  \n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  model_data[save_model_name] = {\n",
    "      'epochs_list' : epochs_list, \n",
    "      'train_losslist': train_losslist, \n",
    "      'valid_losslist' : valid_losslist\n",
    "    }\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_SGD_'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n",
    "    \n",
    "    \n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "  \n",
    "  # Specifying the loss function\n",
    "  optimizer =optim.SGD(model.parameters(), lr=.001,momentum = 0.9)\n",
    "  \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_SGD_Momentum'+model_name+'.pt'\n",
    "  print('################################')\n",
    "  print('Training ',save_model_name,'...')\n",
    "  print('################################')\n",
    "\n",
    "  epochs_list, train_losslist, valid_losslist, model = trainNet(model,criterion,n_epochs,flag_cuda,save_model_name,optimizer,train_loader,test_loader)\n",
    "  \n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  model_data[save_model_name] = {\n",
    "      'epochs_list' : epochs_list, \n",
    "      'train_losslist': train_losslist, \n",
    "      'valid_losslist' : valid_losslist\n",
    "    }\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_SGD_Momentum'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n",
    "    \n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "  \n",
    "  # Specifying the loss function\n",
    "  optimizer = optim.RMSprop(model.parameters(), lr=.001,alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "  \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_RMSprop'+model_name+'.pt'\n",
    "  print('################################')\n",
    "  print('Training ',save_model_name,'...')\n",
    "  print('################################')\n",
    "\n",
    "  epochs_list, train_losslist, valid_losslist, model = trainNet(model,criterion,n_epochs,flag_cuda,save_model_name,optimizer,train_loader,test_loader)\n",
    "  \n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  model_data[save_model_name] = {\n",
    "      'epochs_list' : epochs_list, \n",
    "      'train_losslist': train_losslist, \n",
    "      'valid_losslist' : valid_losslist\n",
    "    }\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_RMSprop'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n",
    "\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "  \n",
    "  # Specifying the loss function\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "  \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_Adam'+model_name+'.pt'\n",
    "  print('################################')\n",
    "  print('Training ',save_model_name,'...')\n",
    "  print('################################')\n",
    "\n",
    "  epochs_list, train_losslist, valid_losslist, model = trainNet(model,criterion,n_epochs,flag_cuda,save_model_name,optimizer,train_loader,test_loader)\n",
    "  \n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "  \n",
    "  model_data[save_model_name] = {\n",
    "      'epochs_list' : epochs_list, \n",
    "      'train_losslist': train_losslist, \n",
    "      'valid_losslist' : valid_losslist\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the learning curves\n",
    "legend = []\n",
    "plt.figure(figsize=(20,10))\n",
    "for data in  list(model_data.keys()):\n",
    "  if data.find('Adam') != -1 : \n",
    "    epochs_list= model_data[data]['epochs_list']\n",
    "    train_losslist= model_data[data]['train_losslist']\n",
    "    valid_losslist = model_data[data]['valid_losslist']\n",
    "    plt.plot(epochs_list, train_losslist, epochs_list, valid_losslist)\n",
    "    legend.append(data+' Training')\n",
    "    legend.append(data+' Validation')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(legend,loc ='right')\n",
    "plt.title(\"Performance of Models\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "legend = []\n",
    "for data in  list(model_data.keys()):\n",
    "  if data.find('SGD_Momentum') != -1 : \n",
    "    epochs_list= model_data[data]['epochs_list']\n",
    "    train_losslist= model_data[data]['train_losslist']\n",
    "    valid_losslist = model_data[data]['valid_losslist']\n",
    "    plt.plot(epochs_list, train_losslist, epochs_list, valid_losslist)\n",
    "    legend.append(data+' Training')\n",
    "    legend.append(data+' Validation')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(legend,loc ='right')\n",
    "plt.title(\"Performance of Models\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "\n",
    "legend = []\n",
    "for data in  list(model_data.keys()):\n",
    "  if data.find('RMSProp') != -1 : \n",
    "    epochs_list= model_data[data]['epochs_list']\n",
    "    train_losslist= model_data[data]['train_losslist']\n",
    "    valid_losslist = model_data[data]['valid_losslist']\n",
    "    plt.plot(epochs_list, train_losslist, epochs_list, valid_losslist)\n",
    "    legend.append(data+' Training')\n",
    "    legend.append(data+' Validation')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(legend,loc ='right')\n",
    "plt.title(\"Performance of Models\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "legend = []\n",
    "for data in  list(model_data.keys()):\n",
    "  if data.find('Adam') != -1 and data.find('SGD_Momentum') and data.find('RMSProp') == -1 : \n",
    "    epochs_list= model_data[data]['epochs_list']\n",
    "    train_losslist= model_data[data]['train_losslist']\n",
    "    valid_losslist = model_data[data]['valid_losslist']\n",
    "    plt.plot(epochs_list, train_losslist, epochs_list, valid_losslist)\n",
    "    legend.append(data+' Training')\n",
    "    legend.append(data+' Validation')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(legend,loc ='right')\n",
    "plt.title(\"Performance of Models\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Optimizer SGD\n",
    "\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_SGD_'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print(save_model_name)\n",
    "  print('####################/n/n')\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n",
    "#Optimizer SGD Momentum\n",
    "\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_SGD_Momentum'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print(save_model_name)\n",
    "  print('####################/n/n')\n",
    "\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n",
    "\n",
    "# RMSprop\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_RMSprop'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print(save_model_name)\n",
    "  print('####################/n/n')\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n",
    "\n",
    "\n",
    "\n",
    "# Adam\n",
    "models_all = models()\n",
    "for model_name in models_all:\n",
    "  model = models_all[model_name]\n",
    "  if flag_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "  save_model_name = 'data_code_'+data_code+'_optimizer_Adam_'+model_name+'.pt'\n",
    "  model.load_state_dict(torch.load('./Models_weight/'+save_model_name))\n",
    "  print('####################')\n",
    "  print(save_model_name)\n",
    "  print('####################/n/n')\n",
    "  print('####################')\n",
    "  print('Test')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,test_loader)\n",
    "  print('\\n####################')\n",
    "  print('Train')\n",
    "  print('####################')\n",
    "  assessNet(model,criterion,train_loader)\n",
    "\n",
    "  confusion = evaluate_confusion_matrix(model,test_loader)\n",
    "  test_plot(confusion,['Standing/Walking','Up_Stairs','Down_Stairs', 'Walking on grass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fnDqS1DufXkh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVyjbJVqC3he"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
